Z minula:
- pretrain:
  - `bart-szn-9-derinet-dist-0`
  - `bart-szn-9-derinet-dist-1`
  - `bart-szn-9-derinet-dist-2`
  - `bart-szn-9-pretrain-morp-5-2`

Poznamka:
- zachovat vsechny modely, logy, predikce (už budou použity v diplomce)
- zachovavat modely po pretrainu - delam vzdy pomoci zkopirovani slozky

1. Implementace:
- zachovavat nejlepsi model podle devu
- dopsat mixování "akcesu" a syntetických dat z pipeliny

2. Trénování:
- pretrain:
  - pretrain ciste aspell a syn data v pipeline
    - ``
  - pretrain aspell, dirinet-dist=2  (5:2) (naboostit cpu) - a pak na nich pustit stejne finetuning
  - pretrain aspell, dirinet-dist=2  (1:1) (naboostit cpu) - a pak na nich pustit stejne finetuning
  - pretrain aspell, dirinet-dist=2  (5:1) (naboostit cpu) - a pak na nich pustit stejne finetuning
- finetuning
  - bez aspellu, derinet-dist=0,1,2 (z pretrainingu bez aspellu)
  - z pretrain: aspell, derinet-dist=0, (5:2)
    - finetuning: aspell, bez derinetu
    - finetuning: aspell, derinet-dist=0 (5:2)
  - z pretrain: ciste aspell
    - finetuning: ciste aspell
    - finetuning: aspell, derinet-dist=0 (5:2)
  - z pretrain: ciste aspell
    - finetuning: ciste aspell
    - finetuning: aspell, derinet-dist=0 (5:2)
  - z kroku z pretrain

3. Typicke chyby:
- diakritiku
  - odstranit veskerou diakritiku
  - přidat diakritiku
    - vaha i znaku je 10^(-i) a normalizace
    - uniforme vybrat tolik znaku kolik je počet
    - vybrane nahodne změnit (bacha e, u)

4. Přetagovat data:
- akces-gec
  - ohlidat new lines (3 a vic za 2)
  - dostat m2 file - https://github.com/ufal/errant_czech/blob/czech/errant/commands/m2_to_m2.py 
  - retagovat ziskany m2 file (dat do evaluace)
- pridat retag v evaluaci

5. Dodelat datasety:
- vzit cisty geccc(dev, test) a dodelat tam specificke chyby - 100x, 200x a 500x(jestli to vubec jde) kazdou (vejit se do 30_000)(rejection sampling) 
- vzit m2 soubor a do nej pridelat dalsi typicke chyby - udelat navic cca. 100 dalsich


---
nodeSelector:
      kubernetes.io/hostname: "node-gpu-w15"