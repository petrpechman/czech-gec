- necham dobehnout 0:1 finetuning - hotovo
- lepsi zobrazeni errant evaluace - grafy
  - pridat do errant vyhodnoceni gold_data = TP+FN (jeste se podivat)
  - je to divne (vypisuji)

- dodelat datasety (pridani chyb do akcesu a gecccu)
  - rozdeleny geccc mam pretagovany - hotovo
  - vzit cisty geccc - plain text(dev, test) a dodelat tam specificke chyby - 100x, (200x a 500x) (jestli to vubec jde) kazdou (vejit se do 30_000)(rejection sampling) 
    - hotovo
  - vzit m2 soubor (už s chybami v gecccu, proto m2) a do nej pridelat dalsi typicke chyby - udelat navic cca. 100 dalsich
    - to do: separátní soubory nebo cely? nejspis cely

- experimenty: 
  - jeste nechat bezet pretrainingy
    - bezi
  - spec_errors, aspell+derinet_dist_2 (5:2)
    - bezi
  - vetsi model:
    - zkusit rozbehnout multi GPU
    - large bart - create_model config_big
  - mt5-base (mt5-large)
  

- errant vyhodnoceni na splitu gecccu
  - pretagovat

---
promazat