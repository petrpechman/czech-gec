- tf - accumulace batch_size - pro mt5-large
  - zkusit jeste jednou a prozkoumat
  - pripadne keras 3.0 (RaggedTensory jeste nejsou podporovany -> mozna upravit)
    - gradient accumulation
  - pripadne dodelat gradient accumulation primo
  - bfloat16 funguje pro mt5 - kouknout na specifikaci kartu
  - Adafactor - zkontrolovat - zkusit mensi batch
  - asi by stacila batch -> 12  
  - mozna zmensit max_length (nejdelsi 80)
  - neni vysoka priorita

- debugging kvuli klesaní při finetunningu - mozna chyba v evaluator.py nebo v pipeline.py
  - chyba: spatny learning rate - opraveno

Experimetny:
- multi-GPU - bart-large
- pretraining:
  - sputit evaluaci na geccc - evals-02-pretrain-eval-geccc
- finetuning:
  - finetuning akces - aspell, derinet2, spec_errs, all
    - [evals-all-0-1](../code/src/evals-all-0-1)
    - [evals-aspell-0-1](../code/src/evals-aspell-0-1)
    - [evals-derinet-dist-2-0-1](../code/src/evals-derinet-dist-2-0-1)
    - [evals-spec-errors-0-1](../code/src/evals-spec-errors-0-1)
  - finetuning geccc - aspell, derinet2, spec_errs, all
    - [evals-02-geccc](../code/src/evals-02-geccc)
    - 60 000 shuffle buffer
    - zjistit jestli neni nekde natvrdo cesta
  - ... dle struktury

- btw. speaking of which, taky muzeme udelat experiment, kdy budeme finetunovat na kazde domene zvlast a ukazeme, jak se to na te kokrentni domene zlepsi (a na ostatnich "zhorsi"), to by mohlo byt super levne (jenom fine-tune) a do diplomky by to mohlo byt dobre

- random sesortit m2 a porovnat, zda se lisi u errantu - i pro ruzne bety: 0.1, 0.5, 1.0, 2.0, 5.0
  - na ceste: src/scripts/data
0.1:
=========== Span-Based Correction ============   =========== Span-Based Correction ============
TP      FP      FN      Prec    Rec     F0.1     TP      FP      FN      Prec    Rec     F0.1
3229    1631    3038    0.6644  0.5152  0.6625   3229    1631    3038    0.6644  0.5152  0.6625
==============================================   ==============================================
0.5:
=========== Span-Based Correction ============   =========== Span-Based Correction ============
TP      FP      FN      Prec    Rec     F0.5     TP      FP      FN      Prec    Rec     F0.5
3229    1631    3038    0.6644  0.5152  0.628    3229    1631    3038    0.6644  0.5152  0.628
==============================================   ==============================================
1.0:
=========== Span-Based Correction ============   =========== Span-Based Correction ============
TP      FP      FN      Prec    Rec     F1.0     TP      FP      FN      Prec    Rec     F1.0
3226    1634    3029    0.6638  0.5157  0.5805   3226    1634    3029    0.6638  0.5157  0.5805
==============================================   ==============================================
2.0:
=========== Span-Based Correction ============   =========== Span-Based Correction ============
TP      FP      FN      Prec    Rec     F2.0     TP      FP      FN      Prec    Rec     F2.0
3224    1636    3025    0.6634  0.5159  0.5399   3220    1640    3018    0.6626  0.5162  0.5401
==============================================   ==============================================
5.0:
=========== Span-Based Correction ============   =========== Span-Based Correction ============
TP      FP      FN      Prec    Rec     F5.0     TP      FP      FN      Prec    Rec     F5.0
3217    1643    3015    0.6619  0.5162  0.5206   3217    1643    3015    0.6619  0.5162  0.5206
==============================================   ==============================================

- pridat Best slovník při volání funkce evaluate_edits

- dodelat datasety:
  - vybirat nahodne z anotatoru pro vytvoreni umeleho datasetu (zamyslet se, jeslti by slo s vice anotatory)
  - vzit cisty geccc - plain text(dev, test) a dodelat tam specificke chyby - 100x, (200x a 500x) (jestli to vubec jde) kazdou (vejit se do 30_000)(rejection sampling) - pro evaluaci
  - vzit m2 soubor(dev, test, train) (už s chybami v gecccu, proto m2) a do nej pridelat dalsi typicke chyby - udelat navic cca. 100 dalsich (200, 500) - evaluace i finetuning 
    - to do: separátní soubory nebo cely? nejspis cely

  - overit, jestli opravdu pouzivam gecccc
  - pustit na nich evaluace


- pretraining - na datasetech 
  - news - spustit
  - cistejsi texty - wikipedie 2022 (v 2. mailu) - bud rozsekat manualne nebo udpipe_tokenizer a plain text
  - spinavejsi texty - common crawl


---

- Overleaf - LaTex
- AJ
- kod - jak je to s zverejnenim? - napsat Milanovi

Struktura:
- Abstract - da se dopsat
- Introduction -> motivace(neco v zadani)
- Related work - co je to za problem, co existuje - pravidlove, neuronkove systemy, dva datasety na cestinu(geccc, akces), metriky na vyhodnoceni, rozebrani problematiky
- Moje cast - viz down
- Conclusion

---

Moje cast:
- proc je 0:1 nejlepsi
- rozebrat pomery -> tabulky, grafy

- porovnani, jaky korpus cistych text -> rozdily
  - syn - done
  - news - dodelat
  - cesky common crawl - zeptat se Milana (chceme porovnat)
    - evaluace na geccc(i split) a akces

- prozkoumat, jak se to chova na specifickych chybach
  - projit to
  - pripadne model, ze umime chyby prumerne a me/mne dobre
  
- reverse pipelina - zminka

- jeslti se vyplaci delat? jestli se neda udelat pomoci chat-gpt (gpt-4) - evaluace na m2
  

---

pocty possible errors:

dev:
[112, 36, 248, 3366, 2976, 4234, 539, 207, 0, 2586, 2, 91, 26, 69, 7, 2921, 98, 2603, 28162, 1153, 119605, 3, 13516, 31792]

test:
[77, 21, 155, 2348, 1781, 2724, 311, 116, 0, 1599, 2, 53, 14, 44, 1, 2508, 308, 781, 17912, 695, 72063, 45, 7977, 20427]

train:
[485, 168, 1412, 17200, 13913, 21906, 2589, 1116, 1, 11465, 14, 514, 85, 292, 13, 22652, 2038, 6510, 138793, 5515, 561701, 101, 60300, 162107]


---
Error:
2024-03-08 11:39:49.400968: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 13634304 totalling 13.00MiB
2024-03-08 11:39:49.400971: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 13635584 totalling 26.01MiB
2024-03-08 11:39:49.400976: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 15730688 totalling 15.00MiB
2024-03-08 11:39:49.400982: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 1024458752 totalling 3.82GiB
2024-03-08 11:39:49.400987: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1035997184 totalling 988.00MiB
2024-03-08 11:39:49.400990: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 13.11GiB
2024-03-08 11:39:49.400996: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 14894563328 memory_limit_: 14894563328 available bytes: 0 curr_region_allocation_bytes_: 29789126656
2024-03-08 11:39:49.401008: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                     14894563328
InUse:                     14075949568
MaxInUse:                  14078070016
NumAllocs:                       10387
MaxAllocSize:               1035997184
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-03-08 11:39:49.401065: W tensorflow/tsl/framework/bfc_allocator.cc:497] ************************************************************************************************____
2024-03-08 11:39:49.401091: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at concat_op.cc:163 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[973465600] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc
Traceback (most recent call last):
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py", line 54, in <module>
    main(args.config, args.eval, args.generate, args.create, args.old, args.version)
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py", line 35, in main
    pipeline.main(config_filename)
  File "/pechmanp/czech-gec/code/src/pipeline/pipeline.py", line 315, in main
    model.fit(
  File "/root/miniconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL ResourceExhaustedError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>

At:
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/errors_impl.py(377): __init__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py(52): quick_execute
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py(381): call
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py(1757): _call_flat
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py(143): __call__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(959): _call
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(894): __call__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler
  /root/miniconda3/lib/python3.11/site-packages/keras/engine/training.py(1685): fit
  /root/miniconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py(65): error_handler
  /pechmanp/czech-gec/code/src/pipeline/pipeline.py(315): main
  /pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py(35): main
  /pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py(54): <module>

Process Process-2:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../utils/load_data.py", line 162, in data_generator
    process_file_in_chunks(
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../utils/load_data.py", line 142, in process_file_in_chunks
    pool.starmap(data_loader, arguments)
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 375, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 768, in get
    self.wait(timeout)
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 765, in wait
    self._event.wait(timeout)
  File "/root/miniconda3/lib/python3.11/threading.py", line 622, in wait
    signaled = self._cond.wait(timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.11/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt


---
cd /pechmanp/czech-gec/code/src/utils/udpipe_tokenizer
cat ../../../data/wiki/damuel_1.0_cs/wiki-cs.txt | python udpipe_tokenizer.py -n cs > wiki-cs-tokenized.txt
cat ../../../data/crawl/Czech/cs-common-crawl.txt | python udpipe_tokenizer.py -n cs > cs-common-crawl-tokenized.txt


75274502 cs-common-crawl-tokenized.txt