- tf - accumulace batch_size - pro mt5-large
  - zkusit jeste jednou a prozkoumat
  - pripadne keras 3.0 (RaggedTensory jeste nejsou podporovany -> mozna upravit)
    - gradient accumulation
  - pripadne dodelat gradient accumulation primo
  - bfloat16 funguje pro mt5 - kouknout na specifikaci kartu
  - Adafactor - zkontrolovat - zkusit mensi batch
  - asi by stacila batch -> 12  
  - mozna zmensit max_length (nejdelsi 80)
  - neni vysoka priorita

- pridat Best slovník při volání funkce evaluate_edits

- pretraining - na datasetech 
  - news - spustit
  - cistejsi texty - wikipedie 2022 (v 2. mailu) - bud rozsekat manualne nebo udpipe_tokenizer a plain text
  - spinavejsi texty - common crawl

Experimetny:
- multi-GPU - bart-large
- pretraining:
  - sputit evaluaci na geccc - evals-02-pretrain-eval-geccc
- finetuning:
  - finetuning akces - aspell, derinet2, spec_errs, all
    - [evals-all-0-1](../code/src/evals-all-0-1)
    - [evals-aspell-0-1](../code/src/evals-aspell-0-1)
    - [evals-derinet-dist-2-0-1](../code/src/evals-derinet-dist-2-0-1)
    - [evals-spec-errors-0-1](../code/src/evals-spec-errors-0-1)
  - finetuning geccc - aspell, derinet2, spec_errs, all
    - [evals-02-geccc](../code/src/evals-02-geccc)
    - 60 000 shuffle buffer
    - zjistit jestli neni nekde natvrdo cesta
  - ... dle struktury

- zjisteni, co se deje na gecccu
  - shuffle 60_000
  - pomer 2:1
  - koukat na data
  - split trainu na domeny 
    - R: 1x25k
    - SL: 1x30k
    - NF: 6x4k
    - NWI: 4x7k
    - [evals-geccc](../code/src/evals-geccc)

- dodelat datasety:
  - overit, jestli opravdu pouzivam gecccc
  - pustit na nich evaluace

---
Error:
2024-03-08 11:39:49.400968: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 13634304 totalling 13.00MiB
2024-03-08 11:39:49.400971: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 13635584 totalling 26.01MiB
2024-03-08 11:39:49.400976: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 15730688 totalling 15.00MiB
2024-03-08 11:39:49.400982: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 1024458752 totalling 3.82GiB
2024-03-08 11:39:49.400987: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1035997184 totalling 988.00MiB
2024-03-08 11:39:49.400990: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 13.11GiB
2024-03-08 11:39:49.400996: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 14894563328 memory_limit_: 14894563328 available bytes: 0 curr_region_allocation_bytes_: 29789126656
2024-03-08 11:39:49.401008: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                     14894563328
InUse:                     14075949568
MaxInUse:                  14078070016
NumAllocs:                       10387
MaxAllocSize:               1035997184
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-03-08 11:39:49.401065: W tensorflow/tsl/framework/bfc_allocator.cc:497] ************************************************************************************************____
2024-03-08 11:39:49.401091: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at concat_op.cc:163 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[973465600] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc
Traceback (most recent call last):
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py", line 54, in <module>
    main(args.config, args.eval, args.generate, args.create, args.old, args.version)
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py", line 35, in main
    pipeline.main(config_filename)
  File "/pechmanp/czech-gec/code/src/pipeline/pipeline.py", line 315, in main
    model.fit(
  File "/root/miniconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py", line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: pybind11::error_already_set: MISMATCH of original and normalized active exception types: ORIGINAL ResourceExhaustedError REPLACED BY KeyboardInterrupt: <EMPTY MESSAGE>

At:
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/framework/errors_impl.py(377): __init__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py(52): quick_execute
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py(381): call
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py(1757): _call_flat
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py(143): __call__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(959): _call
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py(894): __call__
  /root/miniconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py(150): error_handler
  /root/miniconda3/lib/python3.11/site-packages/keras/engine/training.py(1685): fit
  /root/miniconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py(65): error_handler
  /pechmanp/czech-gec/code/src/pipeline/pipeline.py(315): main
  /pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py(35): main
  /pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../pipeline/run.py(54): <module>

Process Process-2:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../utils/load_data.py", line 162, in data_generator
    process_file_in_chunks(
  File "/pechmanp/czech-gec/code/src/mt5-large-01-pretrain/../utils/load_data.py", line 142, in process_file_in_chunks
    pool.starmap(data_loader, arguments)
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 375, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 768, in get
    self.wait(timeout)
  File "/root/miniconda3/lib/python3.11/multiprocessing/pool.py", line 765, in wait
    self._event.wait(timeout)
  File "/root/miniconda3/lib/python3.11/threading.py", line 622, in wait
    signaled = self._cond.wait(timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.11/threading.py", line 320, in wait
    waiter.acquire()
KeyboardInterrupt


---
cd /pechmanp/czech-gec/code/src/utils/udpipe_tokenizer
cat ../../../data/wiki/damuel_1.0_cs/wiki-cs.txt | python udpipe_tokenizer.py -n cs > wiki-cs-tokenized.txt
cat ../../../data/crawl/Czech/cs-common-crawl.txt | python udpipe_tokenizer.py -n cs > cs-common-crawl-tokenized.txt
